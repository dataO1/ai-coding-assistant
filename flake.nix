{
  description = "AI Coding Assistant with LangChain + MCP";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    home-manager.url = "github:nix-community/home-manager";
    home-manager.inputs.nixpkgs.follows = "nixpkgs";
  };

  outputs = { self, nixpkgs, home-manager, ... }:
    let
      lib = nixpkgs.lib;
      system = "x86_64-linux";
      pkgs = nixpkgs.legacyPackages.${system};

      # Python environment with all dependencies
      pythonEnv = pkgs.python311.withPackages (ps: with ps; [
        langchain
        langchain-community
        langchain-openai
        fastapi
        uvicorn
        pydantic
        pydantic-settings
        langgraph
      ]);

      # ========================================================================
      # Build AI Agent Runtime Package (System Level)
      # ========================================================================

      aiAgentRuntime = pkgs.stdenv.mkDerivation {
        name = "ai-agent-runtime";
        src = ./ai-agent-runtime;  # Python runtime code (see below)

        buildInputs = [ pythonEnv ];

        installPhase = ''
          mkdir -p $out/lib/ai-agent
          mkdir -p $out/bin

          # Copy all runtime code
          cp -r . $out/lib/ai-agent/

          # Create launcher script
          cat > $out/bin/ai-agent-server << 'EOF'
          #!/usr/bin/env bash
          export PYTHONPATH="$out/lib/ai-agent:$PYTHONPATH"
          exec ${pythonEnv}/bin/python "$out/lib/ai-agent/server.py" "$@"
          EOF
          chmod +x $out/bin/ai-agent-server
        '';
      };

      # ========================================================================
      # NixOS Module
      # ========================================================================

      aiCodingAssistantModule = { config, lib, pkgs, ... }:
        let
          cfg = config.aiCodingAssistant;
        in
        {
          options.aiCodingAssistant = {
            enable = lib.mkEnableOption "AI Agent infrastructure";

            gpuAcceleration = lib.mkOption {
              type = lib.types.bool;
              default = true;
            };

            ollamaHost = lib.mkOption {
              type = lib.types.str;
              default = "127.0.0.1";
            };

            ollamaPort = lib.mkOption {
              type = lib.types.port;
              default = 11434;
            };

            agentServerPort = lib.mkOption {
              type = lib.types.port;
              default = 8080;
            };

            models = lib.mkOption {
              type = lib.types.attrsOf lib.types.str;
              default = {
                supervisor = "qwen2.5-coder:7b";
                code = "qwen2.5-coder:14b";
                research = "qwen2.5-coder:70b";
              };
            };

            mcpServers = lib.mkOption {
              type = lib.types.attrsOf (lib.types.submodule {
                options = {
                  enable = lib.mkOption { type = lib.types.bool; default = true; };
                  command = lib.mkOption { type = lib.types.str; };
                  args = lib.mkOption { type = lib.types.listOf lib.types.str; default = []; };
                };
              });
              default = {
                filesystem = {
                  enable = true;
                  command = "${pkgs.nodejs}/bin/npx";
                  args = [ "-y" "@modelcontextprotocol/server-filesystem" "/home" ];
                };
                git = {
                  enable = true;
                  command = "${pkgs.nodejs}/bin/npx";
                  args = [ "-y" "@modelcontextprotocol/server-git" ];
                };
              };
            };
          };

          config = lib.mkIf cfg.enable {
            # Ollama service
            services.ollama = {
              enable = true;
              acceleration = if cfg.gpuAcceleration then "cuda" else null;
              host = cfg.ollamaHost;
              port = cfg.ollamaPort;
              loadModels = lib.attrValues cfg.models;
              environmentVariables = {
                OLLAMA_NUM_PARALLEL = "4";
                OLLAMA_MAX_LOADED_MODELS = "3";
              };
            };

            # Agent server systemd service (uses system package)
            systemd.services.ai-agent-server = {
              description = "AI Agent Server (Multi-agent Orchestrator)";
              after = [ "ollama.service" "network-online.target" ];
              wants = [ "network-online.target" ];
              requires = [ "ollama.service" ];
              wantedBy = [ "multi-user.target" ];

              environment = {
                OLLAMA_BASE_URL = "http://${cfg.ollamaHost}:${toString cfg.ollamaPort}";
                AGENT_SERVER_PORT = toString cfg.agentServerPort;
                # Point to user's config (generated by Home Manager)
                AI_AGENT_MANIFESTS = "%h/.config/ai-agent/manifests.json";
                PYTHONUNBUFFERED = "1";
              };

              serviceConfig = {
                Type = "simple";
                # Use the package's launcher script
                ExecStart = "${aiAgentRuntime}/bin/ai-agent-server";
                Restart = "on-failure";
                RestartSec = "10s";
                StandardOutput = "journal";
                StandardError = "journal";
              };
            };

            networking.firewall.allowedTCPPorts = [ cfg.agentServerPort ];

            environment.systemPackages = with pkgs; [
              nodejs
              git
              curl
              aiAgentRuntime  # ← Runtime available system-wide
              pythonEnv
            ] ++ lib.optionals cfg.gpuAcceleration [
              nvtopPackages.full
            ];
          };
        };
      aiAgentUserModule = import ./home-manager-module/default.nix;

    in
    {
      packages.${system}.ai-agent-runtime = aiAgentRuntime;

      nixosModules.default = aiCodingAssistantModule;
      nixosModules.aiCodingAssistant = aiCodingAssistantModule;

      homeManagerModules.default = aiAgentUserModule;
      homeManagerModules.aiAgent = aiAgentUserModule;
    };
}
